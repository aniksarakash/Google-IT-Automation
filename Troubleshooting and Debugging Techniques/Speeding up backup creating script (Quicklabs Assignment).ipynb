{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you have to use multiprocessing and subprocess module methods to sync the data from /data/prod to /data/prod_backup folder.\n",
    "\n",
    "Hint: os.walk() generates the file names in a directory tree by walking the tree either top-down or bottom-up. This is used to traverse the file system in Python.\n",
    "\n",
    "This script uses Python's built-in multiprocessing module to create a pool of worker processes. It uses os.cpu_count() to determine the number of worker processes, which will be equal to the number of CPUs on your machine. It then uses map() function to apply the backup() function to every directory in the source directory. This will run the rsync operation in parallel for each subdirectory of the source directory, which could speed up the operation if there are a large number of directories and if the operation is I/O-bound.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "\n",
    "src = \"/home/<user_name>/data/prod/\"\n",
    "dest = \"/home/<user_name>/data/prod_backup/\"\n",
    "\n",
    "def backup(dir_name):\n",
    "    src_dir = os.path.join(src, dir_name)\n",
    "    dest_dir = os.path.join(dest, dir_name)\n",
    "    subprocess.call([\"rsync\", \"-arq\", src_dir, dest_dir])\n",
    "\n",
    "with Pool(os.cpu_count()) as p:\n",
    "    p.map(backup, os.listdir(src))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two scripts have slightly different functionalities.\n",
    "\n",
    "The first script:\n",
    "\n",
    "Parallelizes the copying process with a separate rsync call for each directory under src but doesn't go into subdirectories.\n",
    "The second script:\n",
    "\n",
    "Parallelizes the copying process with a separate rsync call for each directory and subdirectory under src.\n",
    "If your goal is to ensure the most efficient use of system resources and you have a deeply nested directory structure, the second script may be better because it leverages parallelism more effectively, by allocating a separate process for each directory and subdirectory.\n",
    "\n",
    "However, the second script might also lead to a higher system load if we have a large number of directories, because it creates a pool with as many workers as there are directories.\n",
    "\n",
    "We could consider modifying the second script to use a number of workers equal to the number of CPU cores (like the first script does), which would be a good trade-off between speed and system load:\n",
    "\n",
    "Finally, note that the efficiency of these scripts also depends on the specific workload and system configuration, so it would be a good idea to test them in your environment to see which one performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from multiprocessing import Pool\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "src = \"{}/data/prod/\".format(os.getenv(\"HOME\"))\n",
    "dest = \"{}/data/prod_backup/\".format(os.getenv(\"HOME\"))\n",
    "\n",
    "def run(folder):\n",
    "    subprocess.call([\"rsync\", \"-arq\", folder, dest])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folders = []\n",
    "    for path, directories, files in os.walk(src):\n",
    "        for name in directories:\n",
    "            folders.append(os.path.join(path, name))\n",
    "\n",
    "    pool = Pool(os.cpu_count())  # use as many workers as there are CPU cores\n",
    "    pool.map(run, folders)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
